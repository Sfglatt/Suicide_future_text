---
title: "Prelim text sentiment analysis"
output: html_notebook
---

```{r Github}
# usethis::create_from_github("https://github.com/Sfglatt/Suicide_text.git",
#                            destdir = "Github/Suicide_text")
```

```{r Packages}
if (!require("dplyr")) {install.packages("dplyr"); require("dplyr")}
if (!require("igraph")) {install.packages("igraph"); require("igraph")}
if (!require("ggraph")) {install.packages("ggraph"); require("ggraph")}
if (!require("ggplot2")) {install.packages("ggplot2"); require("ggplot2")}
if (!require("Matrix")) {install.packages("Matrix"); require("Matrix")}
if (!require("officer")) {install.packages("officer"); require("officer")}
if (!require("openai")) {install.packages("openai"); require("openai")}
if (!require("spacyr")) {install.packages("spacyr"); require("spacyr")}
if (!require("text")) {install.packages("text"); require("text")}
if (!require("textdata")) {install.packages("textdata"); require("textdata")}
if (!require("tidyverse")) {install.packages("tidyverse"); require("tidyverse")}
if (!require("tidytext")) {install.packages("tidytext"); require("tidytext")}
if (!require("tm")) {install.packages("tm"); require("tm")}
if (!require("transforEmotion")) {install.packages("transforEmotion"); require("transforEmotion")}
if (!require("topicmodels")) {install.packages("topicmodels"); require("topicmodels")}
if (!require("readr")) {install.packages("readr"); require("readr")}
if (!require("reshape2")) {install.packages("reshape2"); require("reshape2")}
if (!require("widyr")) {install.packages("widyr"); require("widyr")}
if (!require("wordcloud")) {install.packages("wordcloud"); require("wordcloud")}
```

```{r Functions}
# Bigram visualization function
# More about this in the bigram chunk
count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(123456)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
```

```{r Set up}
# set the participant ID name and file path for all forthcoming files per participant
id_name = "P56" 

file_path <- file.path("Data", id_name)
dir.create(file_path)
```

```{r OpenAI}
Sys.setenv(OPENAI_API_KEY = 'my_key')

# Import mp3
rec_file <- sprintf("%s_MERGED.mp3", id_name) # Import the participant that was set above
transcript_file <- file.path(file_path, sprintf("%s_Transcription.docx", id_name))
                  
# Generate transcript
transcript <- openai::create_transcription(model = "whisper-1", rec_file)
transcript$text

# Save transcript in the participant folder
read_docx() |>
  body_add_par(value = transcript$text, style = "normal") |>
  print(target = transcript_file)
```

```{r Data}
# Import transcript
Transcript_doc <- read_docx("P56_Transcript.docx")

# Extract text 
text_data <- docx_summary(Transcript_doc) %>%
  filter(content_type == "paragraph") %>%
  pull(text) %>%
  paste(collapse = " ")


# Convert to dataframe
text_df <- data.frame(text = text_data, stringsAsFactors = FALSE)

# To remove / keep punctuation, uncomment and run the below lines:
# text_punc <- punctuate(text_df, 
#                        allowPunctuations = c("-", "?", "'", "\"", ";", ",", ".", "!"))
# use text_punc instead of text_df if choosing to do this

# Unnest tokens
tidy_text <- text_df %>%
  unnest_tokens(word, text)

# Look
print(head(tidy_text))

# Save data in CSV form
# This will automatically append the right participant ID (above) and date
filetype <- ".csv"
name <- paste0(id_name, "_Transcript_raw_")
filename <- paste(name, Sys.Date(), filetype, sep = '')
full_filename <- file.path(file_path, filename)
write.csv(tidy_text, file = full_filename, row.names = FALSE)
```

```{r Clean}
# remove stop words 
data(stop_words)

stop_words_df <- data.frame(word = stop_words)

tidy_text <- tidy_text %>%
  anti_join(stop_words_df)
head(tidy_text)

# Most common words
tidy_text %>%
  count(word, sort = TRUE)
# Visualize ^
tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>%
  # Set filter to whatever limit you want. Since this is only one participant's transcipt, I'm choosing 5.
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

```{r Sentiment}
# Look at sentiment descriptives 

tidytext::get_sentiments("afinn") # 1
AFINN <- get_sentiments("afinn")

tidytext::get_sentiments("bing") 

get_sentiments("bing") %>% 
  count(sentiment)

tidytext::get_sentiments("nrc")   # 1

get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)

# Example - look at positive words in this participants transcript
nrc_positive <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

tidy_text %>%
  inner_join(nrc_positive) %>%
  count(word, sort = TRUE)
# Love, hope, daughter

# Now look at negative
nrc_negative <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

tidy_text %>%
  inner_join(nrc_negative) %>%
  count(word, sort = TRUE)
# far less instances

# Make dataframes with raw text word, the sentiment, and N

# NRC
(nrc_word_counts <- tidy_text %>%
  inner_join(get_sentiments("nrc")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup())
# Visualize ^
(nrc_plot <- nrc_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)) +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "grey"),
    panel.grid.minor = element_line(color = "grey"),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  )

# bing
(bing_word_counts <- tidy_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup())
# Visualize ^
(bing_plot <- bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL) +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "grey"),
    panel.grid.minor = element_line(color = "grey"),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ))

nrc_plot

# Save the descriptive sentiment visualization
# This will automatically append the right participant ID (set above) and save to that ID's folder
dir.create(file.path("Data", id_name), showWarnings = FALSE, recursive = TRUE)
file_path_nrc <- file.path("Data", id_name, paste0(id_name, "_sentiment_fig_nrc.png"))
file_path_bing <- file.path("Data", id_name, paste0(id_name, "_sentiment_fig_bing.png"))
ggsave(nrc_plot, filename = file_path_nrc)
ggsave(bing_plot, filename = file_path_bing)
```

```{r Sentiment score plot}
# Plot the sentiment score for chunks of words (e.g., every 100 words)

# First, divide the text into chunks
tidy_text <- tidy_text %>%
  mutate(word_count = row_number(),
         index = (word_count - 1) %/% 100 # change this number to how many words per chunk you want
         + 1)

# Plot the score using the different dictionaries

# bing
bing_tidy <- tidy_text %>% 
  inner_join(get_sentiments("bing")) %>% 
  mutate(method = "Bing et al.") %>%
   count(method, index, sentiment) %>% 
   pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>%
  mutate(sentiment = positive - negative) # this makes the sentiment score 

ggplot(bing_tidy, aes(x = index, y = sentiment, fill = factor(1))) +
  geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
  labs(title = "Sentiment over 100 words of text",
       x = "Chunk",
       y = "Sentiment score (bing)") +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "grey"),
        panel.grid.minor = element_line(color = "grey"),
        panel.background = element_rect(fill = "white"),
        plot.background = element_rect(fill = "white"))

# nrc
(nrc_tidy <- tidy_text %>% 
  inner_join(get_sentiments("nrc") %>%
               # only get positive and negative sentiments
               filter(sentiment %in% c("positive", "negative"))) %>% 
    mutate(method = "nrc") %>%
  count(method, index, sentiment) %>% 
   pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>%
  mutate(sentiment = positive - negative))

ggplot(nrc_tidy, aes(index, sentiment, fill = method)) +
  geom_col() +
  labs(title = "Sentiment over 100 words of text",
       x = "Chunk",
       y = "Sentiment score (nrc)") +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "grey"),
        panel.grid.minor = element_line(color = "grey"),
        panel.background = element_rect(fill = "white"),
        plot.background = element_rect(fill = "white"))

# afinn
(afinn_tidy <- tidy_text %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN"))

ggplot(afinn_tidy, aes(index, sentiment, fill = method)) +
  geom_col() +
  labs(title = "Sentiment over 100 words of text",
       x = "Chunk",
       y = "Sentiment score (AFINN)") +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "grey"),
        panel.grid.minor = element_line(color = "grey"),
        panel.background = element_rect(fill = "white"),
        plot.background = element_rect(fill = "white"))

# Look at sentiment scores for all methods
all_dictionaries <- bind_rows(afinn_tidy,
                              bing_tidy, 
                              nrc_tidy)

(sentiment_chunks_plot <- ggplot(all_dictionaries, aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "grey"),
        panel.grid.minor = element_line(color = "grey"),
        panel.background = element_rect(fill = "white"),
        plot.background = element_rect(fill = "white")) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  ggtitle("Sentiment over chunks of text with different lexicons"))

file_path_sentiment_chunks <- file.path("Data", id_name, paste0(id_name, "_sentiment_fig_chunks.png"))
ggsave(sentiment_chunks_plot, filename = file_path_sentiment_chunks)
```

```{r Word cloud}
# Basic
tidy_text %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

# Split by positive and negative
tidy_text %>%
  anti_join(stop_words) %>% # if you want to filter out stop words
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#666666", "#009999"),
                   max.words = 100)

file_path_bing <- file.path("Data", id_name, paste0(id_name, "_wordcloud_bing.png"))
png(filename = file_path_bing)
tidy_text %>%
  anti_join(stop_words) %>% # if you want to filter out stop words
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#666666", "#009999"),
                   max.words = 100)
dev.off()
```

```{r Sentences}
# Tokenize into sentences
text_sentences <- tibble(text = text_data) %>% 
  unnest_tokens(sentence, text, token = "sentences")

# Save sentnces in CSV form
# This will automatically append the right participant ID (above) and date
filetype <- ".csv"
name <- paste0(id_name, "_Transcript_sentences_")
filename <- paste(name, Sys.Date(), filetype, sep = '')
full_filename <- file.path(file_path, filename)
write.csv(text_sentences, file = full_filename, row.names = FALSE)
```

```{r Bigrams and trigrams}
# tokenize into consecutive sequences of words, called n-grams (how often word B follows word A)

(text_data_bigrams <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  # when n = 2 } looking at pairs of 2 consecutive words (called 'bigrams')
  # when n = 3 } looking at pairs of 3 consecutive words (called 'trigrams')
  # etc.
  filter(!is.na(bigram))) 

# Most common bigrams
text_data_bigrams %>%
  count(bigram, sort = TRUE)

# There are a lot of filler / stop words . We can filter this out: 
bigrams_separated <- text_data_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
(bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE))
# "successful"-"teacher"; "daughter"-"leaving"

(bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " "))

# trigrams
# removing stop words
(text_data_trigrams <- text_df %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  filter(!is.na(trigram)) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE))
# "beliefs"-"radically"-"changing", etc.

# If you want to see all the words that come before another word (e.g., what words commonly appear before / after "mental"?)
bigrams_filtered %>%
  filter(word2 == "mental") %>%
  count(word1, sort = TRUE)
# biggest, drastically, etc.

bigrams_separated %>%
  filter(word1 == "mental") %>%
  count(word1, word2, sort = TRUE)

# Look at the most frequent words that were preceded by another word (e.g., “not”) and were associated with a sentiment.
# This is important because while a positive word is given a positive score, if it is preceded by a negating word like not, it's not used in a positive way
(not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE))
# easy, which has a positive score (Value) of 1, was said as "not easy"

# Visualize the words that contributed the most in the “wrong” direction. 
# Multiply their value by the number of times they appear (so that a word with a value of +3 occurring 10 times has as much impact as a word with a sentiment value of +1 occurring 30 times). 
# [In this case it's only one instance, so the figure isn't necessary]
not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")

# Do this with other negating words, like "no"
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

negated_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"negation\"")

# Visualize bigram
# first, filter for common combinations. Since this is only one participant, I'm doing n > 0.
(bigram_graph <- bigram_counts %>%
  filter(n > 0) %>%
  graph_from_data_frame())

set.seed(123456)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

# To do all of cleaning --> visualization that is done step-by-step in one, use the functions from the functions chunk:
text_bigrams <- text_df %>%
  count_bigrams()

text_bigrams %>%
  filter(n > 0,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()
```

```{r Document-term matrix format}
# Change into document-term matrix format
(Transcript_dtm <- tidy_text %>%
  count(document = 1, word) %>%
  cast_dtm(document, word, n))
```

```{r Topic modeling}
# PSA: Divide text into natural groups to understand them separately. Topic modeling is a method for unsupervised classification, similar to clustering on numeric data, which finds natural groups of items 
# Latent Dirichlet allocation (LDA) is a way of doing ^^.

(Transcript_lda <- LDA(Transcript_dtm, 
                       k = 2,
                       # The number of 'topics' you want to model
                       control = list(seed = 123456)))
(LDA_topics <- tidy(Transcript_lda, matrix = "beta"))

top_terms <- LDA_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
# look at the terms in each topic (cluser 1/2). The terms are allowed to overlap. This is a good thing - an advantage of topic modeling (vs “hard clustering” methods) - since topics used in natural language  have  overlap in terms of words

# Look at the words with the *greatest difference between topics* via log ratio
(beta_wide <- LDA_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)))

# limit to 10 largest and smallest log ratios
beta_wide <- bind_rows(
  beta_wide %>% 
    slice_min(log_ratio, n = 10) %>%
    mutate(topic = "topic1"),
  beta_wide %>%
    slice_max(log_ratio, n = 10) %>%
    mutate(topic = "topic2"))

ggplot(beta_wide, aes(x = log_ratio, y = term))+
  geom_col()+
  facet_grid(rows = vars(topic), scales = "free_y")+
  labs(x = "Log Ratio", y = "Term")
```

```{r Emoxicon}

(emoxicon_scores <- transforEmotion::emoxicon_scores(tidy_text$word
                                                     # needs to be in the 1 per row form
                                                     # Using the default language - 
                                                     # modification of the lexicon of Araque et al., (2018).
                                                     # exclude
                                                     ))
```

```{r SpaceyR}
# Not particularly useful for study Qs

spacy_install()

spacy_initialize(model = "en_core_web_sm")

(parsedtxt <- spacy_parse(text_df$text)) 

# process documents and get a data table
spacy_parse(text_df$text, tag = TRUE, entity = FALSE, lemma = FALSE)

# Parse texts
spacy_tokenize(text_df$text)

# Output a dataframe
spacy_tokenize(txt, remove_punct = TRUE, output = "data.frame") %>%
    tail()

### Extract language properties 
# Extract entities; “Extended” entities include entities like dates, events, and cardinal or ordinal quantities
parsedtxt <- spacy_parse(text_df$text, lemma = FALSE, entity = TRUE, nounphrase = TRUE)
entity_extract(parsedtxt)
# compound multi-word things into single entities
entity_consolidate(parsedtxt) %>%
    tail()

# Extract nouns
nounphrase_extract(parsedtxt)
# Compount multi-word things to single tokens
nounphrase_consolidate(parsedtxt) 

spacy_finalize()
```